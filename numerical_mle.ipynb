{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasondupree/jasondupree.github.io/blob/main/numerical_mle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqSTMdTseiTw"
      },
      "source": [
        "# EX2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCAc46EeeiT1"
      },
      "source": [
        "All of this work should be done within a Jupyter notebook. Please format it consistently with the notebooks that we used during the course. In particular, place emphasis on making your comments, code, and plots readable.\n",
        "Turn in both the .ipynb file and a .pdf version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SByMJeiXeiT3"
      },
      "source": [
        "## Question One\n",
        "Let X1, X2, ... Xn be i.i.d. from the Beta(a; b) distribution. Find numerically the maximum likelihood estimators for a and b. Mimic the function gammamle() that we created in\n",
        "lecture.\n",
        "The function should also return the covariance matrix for the MLE.\n",
        "Print the function that you wrote and attach it to your homework, and also demonstrate that\n",
        "it works by using a few simulations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model. Given a set of data and a statistical model with parameters, MLE finds the parameter values that maximize the likelihood function, which measures how likely it is to observe the given data under different parameter values.\n",
        "\n",
        "Step-by-Step Process\n",
        "\n",
        "    Define the likelihood function:\n",
        "    The likelihood function for a normal distribution is based on its probability density function (PDF). For a set of observations, the log-likelihood is often used for numerical stability.\n",
        "\n",
        "    Use an optimization algorithm:\n",
        "    To maximize the log-likelihood function, we use an optimization algorithm like scipy.optimize.minimize.\n",
        "\n",
        "    Estimate the parameters:\n",
        "    Provide an initial guess for the parameters and use the optimization algorithm to find the parameter values that maximize the log-likelihood."
      ],
      "metadata": {
        "id": "jG_1QyLvjONw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import beta\n",
        "from scipy.special import gammaln, psi\n",
        "from numpy.linalg import inv\n",
        "\n",
        "def beta_log_likelihood(params, data):\n",
        "    a, b = params\n",
        "    if a <= 0 or b <= 0:\n",
        "        return np.inf  # Return infinity if parameters are non-positive\n",
        "    n = len(data)\n",
        "    log_likelihood = (\n",
        "        n * (gammaln(a + b) - gammaln(a) - gammaln(b))\n",
        "        + (a - 1) * np.sum(np.log(data))\n",
        "        + (b - 1) * np.sum(np.log(1 - data))\n",
        "    )\n",
        "    return -log_likelihood  # Negative because we will minimize\n",
        "\n",
        "def beta_mle(data):\n",
        "    initial_guess = [1, 1]  # Initial guess for a and b\n",
        "    result = minimize(beta_log_likelihood, initial_guess, args=(data,), method='L-BFGS-B', bounds=[(1e-6, None), (1e-6, None)])\n",
        "    a_mle, b_mle = result.x\n",
        "\n",
        "    # Compute the observed Fisher Information Matrix (Hessian)\n",
        "    hessian_inv = result.hess_inv.todense()\n",
        "\n",
        "    return a_mle, b_mle, hessian_inv\n",
        "\n",
        "# Test the function with a few simulations\n",
        "np.random.seed(0)\n",
        "data = np.random.beta(2, 5, 1000)\n",
        "a_mle, b_mle, cov_matrix = beta_mle(data)\n",
        "\n",
        "print(f\"Estimated a: {a_mle}\")\n",
        "print(f\"Estimated b: {b_mle}\")\n",
        "print(\"Covariance matrix:\")\n",
        "print(cov_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRDRcMEOjLgN",
        "outputId": "e93d35f7-418e-48b3-b5e8-98583721a5ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated a: 2.05885302056502\n",
            "Estimated b: 4.97025499625221\n",
            "Covariance matrix:\n",
            "[[0.01275145 0.03818154]\n",
            " [0.03818154 0.1410208 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "    Data Generation:\n",
        "        We use np.random.beta to generate sample data from a Beta distribution with known parameters for testing purposes.\n",
        "\n",
        "    Log-Likelihood Function:\n",
        "        The function beta_log_likelihood calculates the negative log-likelihood for the Beta distribution. It takes the parameters and data as input and returns the negative log-likelihood value.\n",
        "\n",
        "    Optimization:\n",
        "        We use scipy.optimize.minimize with the L-BFGS-B method to find the parameter values that minimize the negative log-likelihood function. The bounds ensure that the parameters remain positive.\n",
        "\n",
        "    Fisher Information Matrix:\n",
        "        The covariance matrix of the MLEs is the inverse of the observed Fisher Information Matrix (Hessian). In the case of the L-BFGS-B method, the Hessian inverse is available directly.\n",
        "\n",
        "    Testing:\n",
        "        We test the function with simulated data to ensure it works correctly."
      ],
      "metadata": {
        "id": "awTvK9LJpsAd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO0bYVbyeiT6"
      },
      "source": [
        "## Question Two\n",
        "\n",
        "Create a new treg() function like the one demonstrated in lecture, but now include the\n",
        "number of degrees of freedom for the error distribution as an additional parameter to be\n",
        "estimated via maximum likeihood.\n",
        "Print the function that you wrote and attach it to your homework, and also demonstrate that\n",
        "it works by using a few simulations.\n",
        "\n",
        "The function should also return the covariance matrix for the MLE.\n",
        "\n",
        "Comment: There is nothing wrong with having a non-integer valued number of degrees of\n",
        "freedom with the t-distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A treg() function estimates the parameters of a regression model with t-distributed errors, including the number of degrees of freedom for the error distribution. The function will also return the covariance matrix for the MLE."
      ],
      "metadata": {
        "id": "JVTtFwHfrjJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR1sPKjleiT7",
        "outputId": "bf9bd5c4-6260-4829-c9df-ee51f36d2a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated beta0: 0.6166797416082761\n",
            "Estimated beta1: 1.598952169648803\n",
            "Estimated sigma: 4.596004313853946\n",
            "Estimated nu: 79.78849667802736\n",
            "Covariance matrix:\n",
            "[[ 9.78279058e-01 -1.51233122e-01  9.66007726e-04  7.78666948e-03]\n",
            " [-1.51233122e-01  3.05622361e-02 -2.95575399e-04 -1.00101082e-02]\n",
            " [ 9.66007726e-04 -2.95575399e-04  1.09356395e-01  1.53412437e-02]\n",
            " [ 7.78666948e-03 -1.00101082e-02  1.53412437e-02  8.90588800e-01]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import t\n",
        "from numpy.linalg import inv\n",
        "\n",
        "def neglogliketreg(pars, x, y):\n",
        "    beta0, beta1, sigma, nu = pars\n",
        "    if sigma <= 0 or nu <= 0:\n",
        "        return np.inf\n",
        "    resid = y - (beta0 + beta1 * x)\n",
        "    log_likelihood = np.sum(t.logpdf(resid / sigma, df=nu) - np.log(sigma))\n",
        "    return -log_likelihood  # Negative because we will minimize\n",
        "\n",
        "def treg(x, y):\n",
        "    n = len(x)\n",
        "\n",
        "    # Initial guesses for the parameters\n",
        "    beta1_init = np.cov(x, y)[0, 1] / np.var(x)\n",
        "    beta0_init = np.mean(y) - beta1_init * np.mean(x)\n",
        "    resid = y - (beta0_init + beta1_init * x)\n",
        "    sigma_init = np.sqrt(np.mean(resid ** 2))\n",
        "    nu_init = 5  # Initial guess for degrees of freedom\n",
        "\n",
        "    initial_guess = [beta0_init, beta1_init, sigma_init, nu_init]\n",
        "\n",
        "    # Perform the optimization\n",
        "    result = minimize(neglogliketreg, initial_guess, args=(x, y), method='L-BFGS-B', bounds=[(None, None), (None, None), (1e-6, None), (1e-6, 100)])\n",
        "    beta0_mle, beta1_mle, sigma_mle, nu_mle = result.x\n",
        "\n",
        "    # Compute the observed Fisher Information Matrix (Hessian)\n",
        "    hessian_inv = result.hess_inv.todense()\n",
        "\n",
        "    return (beta0_mle, beta1_mle, sigma_mle, nu_mle), hessian_inv\n",
        "\n",
        "# Test the function with a few simulations\n",
        "np.random.seed(0)\n",
        "x = np.random.uniform(0, 10, 100)\n",
        "y = 2.5 + 1.3 * x + 5 * t.rvs(df=10, size=len(x))\n",
        "\n",
        "params, cov_matrix = treg(x, y)\n",
        "beta0_mle, beta1_mle, sigma_mle, nu_mle = params\n",
        "\n",
        "print(f\"Estimated beta0: {beta0_mle}\")\n",
        "print(f\"Estimated beta1: {beta1_mle}\")\n",
        "print(f\"Estimated sigma: {sigma_mle}\")\n",
        "print(f\"Estimated nu: {nu_mle}\")\n",
        "print(\"Covariance matrix:\")\n",
        "print(cov_matrix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}